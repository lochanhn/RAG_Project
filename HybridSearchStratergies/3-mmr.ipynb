{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcf6a13",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance\n",
    "MMR (Maximal Marginal Relevance) is a powerful diversity-aware retrieval technique used in information retrieval and RAG pipelines to balance relevance and novelty when selecting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87294dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and chunk the document\n",
    "loader = TextLoader(\"langchain_rag_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: FAISS Vector Store with HuggingFace Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Create MMR Retirever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ded4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a54fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prompt and LLM\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "llm = init_chat_model(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: RAG Pipeline\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Query\n",
    "query = {\"input\": \"How does LangChain support agents and memory?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\"âœ… Answer:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e3a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Project (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

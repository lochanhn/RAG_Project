{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a23eff",
   "metadata": {},
   "source": [
    "### Reranking Hybrid Search Statergies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5fac5",
   "metadata": {},
   "source": [
    "Re-ranking is a second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "\n",
    "1. First use a fast retriever (like BM25, FAISS, hybrid) to fetch top-k documents quickly.\n",
    "\n",
    "2. Then use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.\n",
    "\n",
    "ðŸ‘‰ It ensures that the most relevant documents appear at the top, improving the final answer from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load text file\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "\n",
    "# Split text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query\n",
    "query=\"How can i use langchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9720bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS and Huggingface model Embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332edeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Embedding\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore_openai=FAISS.from_documents(docs,embeddings)\n",
    "retriever_openai=vectorstore_openai.as_retriever(search_kwargs={\"k\":8})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e52001",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt and use the llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm = init_chat_model(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eed561",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt| llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3278ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecef652",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9882543",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e76228",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468339a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d31bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nðŸ“Š Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Project (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
